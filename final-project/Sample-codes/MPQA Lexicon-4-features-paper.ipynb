{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    stopword_list=nltk.corpus.stopwords.words('english')\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(file):\n",
    "    data = pd.read_csv(file, sep='\\t', names=[\"id\", \"polarity\", \"tweet\"])\n",
    "    data = data.drop_duplicates()\n",
    "    data['tweet']=data['tweet'].apply(remove_stopwords)\n",
    "    data[\"tweaet\"] = data[\"tweet\"].str.lower() # lowercase\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mpqa_count(tweet, tokenizer):\n",
    "    count_pos = 0\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        token = token.lower()\n",
    "        if token in word_dict.keys():\n",
    "            for value in word_dict[token]:\n",
    "                if value == 'positive':\n",
    "                    count_pos += 1\n",
    "    result_dict = dict()\n",
    "    result_dict['positive'] = count_pos\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_score(tweet, tokenizer):\n",
    "    neg_list = []\n",
    "    pos_list = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        token = token.lower()\n",
    "        if len(word_dict[token]) >= 3:\n",
    "            score = word_dict[token][2]\n",
    "            if score == 1:\n",
    "                pos_list.append(1)\n",
    "            else:\n",
    "                neg_list.append(-1)\n",
    "    return {'total_sum': sum(pos_list) + sum(neg_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_token(tweet, tokenizer):\n",
    "    \n",
    "    neg_list = []\n",
    "    pos_list = []\n",
    "    \n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        token = token.lower()\n",
    "        if len(word_dict[token]) >= 3:\n",
    "            score = word_dict[token][2]\n",
    "            if score == 1:\n",
    "                pos_list.append(1)\n",
    "            else:\n",
    "                neg_list.append(-1)\n",
    "\n",
    "    try:\n",
    "        pos_max = max(pos_list)\n",
    "    except ValueError:\n",
    "        pos_max = 0\n",
    "    try:\n",
    "        neg_max = min(neg_list)\n",
    "    except ValueError:\n",
    "        neg_max = 0\n",
    "        \n",
    "    return {'max' : pos_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token(tweet, tokenizer):\n",
    "    for token in reversed(tokenizer.tokenize(tweet)):\n",
    "        token = token.lower()\n",
    "        if len(word_dict[token]) >= 3:\n",
    "            score = word_dict[token][2]\n",
    "            if score == 1:\n",
    "                return {'last_polarity' : 1}\n",
    "            elif score == -1:\n",
    "                return {'last_polarity' : -1}\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return {'last_polarity' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_feats(tweet, tokenizer):\n",
    "    count_tokens = generate_mpqa_count(tweet, tokenizer)\n",
    "    pol = count_total_score(tweet, tokenizer)\n",
    "    max_tok = max_token(tweet, tokenizer)\n",
    "    last = last_token(tweet, tokenizer)\n",
    "    result = dict()\n",
    "    for dictionary in [count_tokens, pol, max_tok, last]:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(only_tweet_data, tokenizer):\n",
    "    mpqa_counts = [all_feats(tweet, tokenizer) for tweet in only_tweet_data]\n",
    "    mpqa_df = pd.DataFrame(mpqa_counts, index=only_tweet_data.index)\n",
    "    mpqa_df = mpqa_df.fillna(0)\n",
    "    mpqa_np = mpqa_df.to_numpy()\n",
    "    return mpqa_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff', sep='\\t',names=[\"values\"])\n",
    "result = []\n",
    "for i in range(len(data[\"values\"])):\n",
    "    token = data[\"values\"][i].split()\n",
    "    for j in range(6):\n",
    "        token[j] = token[j][token[j].index(\"=\")+1:]\n",
    "    result.append(token)\n",
    "word_dict = defaultdict(list)\n",
    "\n",
    "for word_line in result:\n",
    "    word_dict[word_line[2]] = [word_line[0],word_line[5]]\n",
    "    if word_line[5] == \"negative\":\n",
    "        word_dict[word_line[2]].append(-1)\n",
    "    elif word_line[5] == \"positive\":\n",
    "        word_dict[word_line[2]].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "train_data = clean_df(\"./data/dataset/twitter-2013train-A.txt\")\n",
    "only_tweet_train_data = train_data['tweet']\n",
    "train_mpqa_feature = get_feature(only_tweet_train_data, tweet_tokenizer)\n",
    "\n",
    "train_labels = train_data.polarity\n",
    "result = []\n",
    "for x in train_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "train_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "train_mpqa_feature = scaler.fit_transform(train_mpqa_feature)\n",
    "train_features = np.array(train_mpqa_feature)\n",
    "\n",
    "print(\"train labels: \", train_labels) \n",
    "print(\"train features:\", train_features) \n",
    "print(\"train labels shape: \", train_labels.shape) \n",
    "print(\"train features shape:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "dev_data = clean_df(\"./data/dataset/twitter-2013dev-A.txt\")\n",
    "only_tweet_dev_data = dev_data['tweet']\n",
    "dev_mpqa_feature = get_feature(only_tweet_dev_data, tweet_tokenizer)\n",
    "\n",
    "dev_labels = dev_data.polarity\n",
    "result = []\n",
    "for x in dev_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "dev_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "dev_mpqa_feature = scaler.fit_transform(dev_mpqa_feature)\n",
    "dev_features = np.array(dev_mpqa_feature)\n",
    "\n",
    "print(\"dev labels: \", dev_labels) \n",
    "print(\"dev features:\", dev_features) \n",
    "print(\"dev_labels shape: \", dev_labels.shape) \n",
    "print(\"dev_features shape:\", dev_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "param_grid = {'C': [0.005, 0.1, 0.5, 1], \n",
    "              'kernel': ['linear','rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "grid.fit(train_features, train_labels)\n",
    "\n",
    "print(grid.best_params_)\n",
    "  \n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(dev_features)\n",
    "  \n",
    "print(classification_report(dev_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "dev_data = clean_df(\"./data/dataset/twitter-2013test-A.txt\")\n",
    "only_tweet_dev_data = dev_data['tweet']\n",
    "dev_mpqa_feature = get_feature(only_tweet_dev_data, tweet_tokenizer)\n",
    "\n",
    "dev_labels = dev_data.polarity\n",
    "result = []\n",
    "for x in dev_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "dev_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "dev_mpqa_feature = scaler.fit_transform(dev_mpqa_feature)\n",
    "dev_features = np.array(dev_mpqa_feature)\n",
    "\n",
    "print(\"dev labels: \", dev_labels) \n",
    "print(\"dev features:\", dev_features) \n",
    "print(\"dev_labels shape: \", dev_labels.shape) \n",
    "print(\"dev_features shape:\", dev_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"mpaq_lexicon_less_features_paper_vector.txt\", \"w+\")\n",
    "for i in dev_mpqa_feature:\n",
    "    content = str(i)\n",
    "    file.write(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "param_grid = {'C': [0.005, 0.1, 0.5, 1], \n",
    "              'kernel': ['linear','rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "grid.fit(train_features, train_labels)\n",
    "\n",
    "print(grid.best_params_)\n",
    "  \n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(dev_features)\n",
    "  \n",
    "print(classification_report(dev_labels, grid_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
