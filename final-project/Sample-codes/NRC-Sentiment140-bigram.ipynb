{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    stopword_list=nltk.corpus.stopwords.words('english')\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(file):\n",
    "    data = pd.read_csv(file, sep='\\t', names=[\"id\", \"polarity\", \"tweet\"])\n",
    "    data = data.drop_duplicates()\n",
    "    data['tweet']=data['tweet'].apply(remove_stopwords)\n",
    "    data[\"tweet\"] = data[\"tweet\"].str.lower() # lowercase\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(n_gram):\n",
    "    score = word_dict[n_gram]\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    if score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_polarity_tokens(tweet, tokenizer):\n",
    "    \n",
    "    score_list = []\n",
    "    tokenized = tokenizer.tokenize(tweet)\n",
    "    ngrams_list = [' '.join(i) for i in ngrams(tokenized, 2)]\n",
    "    for ngram in ngrams_list:\n",
    "        ngram = ngram.lower()\n",
    "        score = polarity(ngram)\n",
    "        scorelist.append(score)\n",
    "        \n",
    "    return dict(Counter(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_polarity(tweet, tokenizer):\n",
    "    neg_list = []\n",
    "    pos_list = []\n",
    "    tokenized = tokenizer.tokenize(tweet)\n",
    "    ngrams_list = [' '.join(i) for i in ngrams(tokenized, 2)]\n",
    "    for ngram in ngrams_list:\n",
    "        ngram = ngram.lower()\n",
    "        if polarity(ngram) == 'positive':\n",
    "            pos_list.append(word_dict[ngram])\n",
    "        elif polarity(ngram) == 'negative':\n",
    "            neg_list.append(abs(word_dict[ngram]))\n",
    "        \n",
    "    return {'pos_sum' : sum(pos_list), 'neg_sum' : sum(neg_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_token(tweet, tokenizer):\n",
    "    \n",
    "    neg_list = []\n",
    "    pos_list = []\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(tweet)\n",
    "    ngrams_list = [' '.join(i) for i in ngrams(tokenized, 2)]\n",
    "    for ngram in ngrams_list:\n",
    "        ngram = ngram.lower()\n",
    "        if polarity(ngram) == 'positive':\n",
    "            pos_list.append(word_dict[ngram])\n",
    "        elif polarity(ngram) == 'negative':\n",
    "            neg_list.append(word_dict[ngram])\n",
    "\n",
    "    try:\n",
    "        pos_max = max(pos_list)\n",
    "    except ValueError:\n",
    "        pos_max = 0\n",
    "    try:\n",
    "        neg_max = min(neg_list)\n",
    "    except ValueError:\n",
    "        neg_max = 0\n",
    "        \n",
    "    return {'pos_max' : pos_max, 'neg_max' : neg_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token(tweet, tokenizer):\n",
    "    tokenized = tokenizer.tokenize(tweet)\n",
    "    ngrams_list = [' '.join(i) for i in ngrams(tokenized, 2)]\n",
    "    \n",
    "    for token in reversed(ngrams_list):\n",
    "        token = token.lower()\n",
    "        if polarity(token) == 'positive' or polarity(token) == 'negative':\n",
    "            return {'last_polarity' : word_dict[token]}\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return {'last_polarity' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_feats(tweet, tokenizer):\n",
    "    count_tokens = count_polarity_tokens(tweet, tokenizer)\n",
    "    polarity = sum_polarity(tweet, tokenizer)\n",
    "    max_tok = max_token(tweet, tokenizer)\n",
    "    last_tok = last_token(tweet, tokenizer)\n",
    "    \n",
    "    result = dict()\n",
    "    for dictionary in [count_tokens, polarity, max_tok, last_tok]:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(only_tweet_data, tokenizer):\n",
    "    sentiment140_counts = [all_feats(tweet, tokenizer) for tweet in only_tweet_data]\n",
    "    sentiment140_df = pd.DataFrame(sentiment140_counts, index=only_tweet_data.index)\n",
    "    sentiment140_df = sentiment140_df.fillna(0)\n",
    "    sentiment140_np = sentiment140_df.to_numpy()\n",
    "    return sentiment140_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = defaultdict(float)\n",
    "\n",
    "with open('./data/Sentiment140-Lexicon-v0.1/bigrams-pmilexicon.txt', 'r') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.split()\n",
    "        word_dict[row[0] +\" \" + row[1]] = float(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "train_data = clean_df(\"./data/dataset/twitter-2013train-A.txt\")\n",
    "only_tweet_train_data = train_data['tweet']\n",
    "train_Sentiment140_bi_feature = get_feature(only_tweet_train_data, tweet_tokenizer)\n",
    "\n",
    "train_labels = train_data.polarity\n",
    "result = []\n",
    "for x in train_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "train_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "train_Sentiment140_bi_feature = scaler.fit_transform(train_Sentiment140_bi_feature)\n",
    "train_features = np.array(train_Sentiment140_bi_feature)\n",
    "\n",
    "print(\"train labels: \", train_labels) \n",
    "print(\"train features:\", train_features) \n",
    "print(\"train labels shape: \", train_labels.shape) \n",
    "print(\"train features shape:\", train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "dev_data = clean_df(\"./data/dataset/twitter-2013test-A.txt\")\n",
    "\n",
    "only_tweet_dev_data = dev_data['tweet']\n",
    "dev_Sentiment140_bi_feature = get_feature(only_tweet_dev_data, tweet_tokenizer)\n",
    "\n",
    "dev_labels = dev_data.polarity\n",
    "result = []\n",
    "for x in dev_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "dev_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "dev_Sentiment140_bi_feature = scaler.fit_transform(dev_Sentiment140_bi_feature)\n",
    "dev_features = np.array(dev_Sentiment140_bi_feature)\n",
    "\n",
    "print(\"dev labels: \", dev_labels) \n",
    "print(\"dev features:\", dev_features) \n",
    "print(\"dev_labels shape: \", dev_labels.shape) \n",
    "print(\"dev_features shape:\", dev_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"sentiment140_bigram_vector.txt\", \"w+\")\n",
    "for i in dev_Sentiment140_bi_feature:\n",
    "    content = str(i)\n",
    "    file.write(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNegative weight = 3.14 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNegative weight = 3.14 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNegative weight = 3.14 \\nPositive weight = 1.25 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNegative weight = 3.14 \\nPositive weight = 1.25 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNo weight\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNo weight\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "param_grid = {'C': [0.005, 0.1, 0.5, 1], \n",
    "              'kernel': ['linear','rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "grid.fit(train_features, train_labels)\n",
    "\n",
    "print(grid.best_params_)\n",
    "  \n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(dev_features)\n",
    "  \n",
    "print(classification_report(dev_labels, grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "dev_data = clean_df(\"./data/dataset/twitter-2013dev-A.txt\")\n",
    "\n",
    "only_tweet_dev_data = dev_data['tweet']\n",
    "dev_Sentiment140_bi_feature = get_feature(only_tweet_dev_data, tweet_tokenizer)\n",
    "\n",
    "dev_labels = dev_data.polarity\n",
    "result = []\n",
    "for x in dev_labels:\n",
    "    if x == \"positive\":\n",
    "        result.append(2)\n",
    "    elif x == \"negative\":\n",
    "        result.append(1)\n",
    "    elif x == \"neutral\":\n",
    "        result.append(0)\n",
    "dev_labels = np.array(result)\n",
    "scaler = StandardScaler()\n",
    "dev_Sentiment140_bi_feature = scaler.fit_transform(dev_Sentiment140_bi_feature)\n",
    "dev_features = np.array(dev_Sentiment140_bi_feature)\n",
    "\n",
    "print(\"dev labels: \", dev_labels) \n",
    "print(\"dev features:\", dev_features) \n",
    "print(\"dev_labels shape: \", dev_labels.shape) \n",
    "print(\"dev_features shape:\", dev_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNegative weight = 3.14 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "sample_weight = np.array([3.14 if i == 1 else 1 for i in train_labels])\n",
    "\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNegative weight = 3.14 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNegative weight = 3.14 \\nPositive weight = 1.25 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "arr = []\n",
    "\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        arr.append(3.14)\n",
    "    elif i == 2:\n",
    "        arr.append(1.25)\n",
    "    else:\n",
    "        arr.append(1)\n",
    "\n",
    "\n",
    "sample_weight = np.array(arr)\n",
    "clf.fit(train_features, train_labels, sample_weight = sample_weight)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNegative weight = 3.14 \\nPositive weight = 1.25 \\n\\n\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=1 \\nNo weight\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.005, probability=True)\n",
    "\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "predictions = clf.predict(dev_features)\n",
    "\n",
    "print(\"A Classification Report showing the per-class Precision, Recall and F1-score \\n\\nC=0.005 \\nNo weight\", metrics.classification_report(dev_labels, predictions,target_names=['neutral','negative','positive']))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "param_grid = {'C': [0.005, 0.1, 0.5, 1], \n",
    "              'kernel': ['linear','rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "grid.fit(train_features, train_labels)\n",
    "\n",
    "print(grid.best_params_)\n",
    "  \n",
    "print(grid.best_estimator_)\n",
    "\n",
    "grid_predictions = grid.predict(dev_features)\n",
    "  \n",
    "print(classification_report(dev_labels, grid_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
